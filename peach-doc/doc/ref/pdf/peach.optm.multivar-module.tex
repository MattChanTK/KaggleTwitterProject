%
% API Documentation for Peach - Computational Intelligence for Python
% Module peach.optm.multivar
%
% Generated by epydoc 3.0.1
% [Sun Jul 31 17:00:41 2011]
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                          Module Description                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \index{peach \textit{(package)}!peach.optm \textit{(package)}!peach.optm.multivar \textit{(module)}|(}
\section{Module peach.optm.multivar}

    \label{peach:optm:multivar}

This package implements basic multivariable optimizers, including gradient and
Newton searches.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               Variables                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Variables}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright \_\-\_\-d\-o\-c\-\_\-\_\- & \raggedright \textbf{Value:} 
{\tt \texttt{...}}&\\
\cline{1-2}
\raggedright \_\-\_\-p\-a\-c\-k\-a\-g\-e\-\_\-\_\- & \raggedright \textbf{Value:} 
{\tt \texttt{'}\texttt{peach.optm}\texttt{'}}&\\
\cline{1-2}
\end{longtable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           Class Description                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \index{peach \textit{(package)}!peach.optm \textit{(package)}!peach.optm.multivar \textit{(module)}!peach.optm.multivar.Direct \textit{(class)}|(}
\subsection{Class Direct}

    \label{peach:optm:multivar:Direct}
\begin{tabular}{cccccccc}
% Line for object, linespec=[False, False]
\multicolumn{2}{r}{\settowidth{\BCL}{object}\multirow{2}{\BCL}{object}}
&&
&&
  \\\cline{3-3}
  &&\multicolumn{1}{c|}{}
&&
&&
  \\
% Line for peach.optm.base.Optimizer, linespec=[False]
\multicolumn{4}{r}{\settowidth{\BCL}{peach.optm.base.Optimizer}\multirow{2}{\BCL}{peach.optm.base.Optimizer}}
&&
  \\\cline{5-5}
  &&&&\multicolumn{1}{c|}{}
&&
  \\
&&&&\multicolumn{2}{l}{\textbf{peach.optm.multivar.Direct}}
\end{tabular}


Multidimensional direct search

This optimization method is a generalization of the 1D method, using
variable swap as search direction. This results in a very simplistic and
inefficient method that should be used only when any other method fails.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                Methods                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Methods}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_init\_\_}(\textit{self}, \textit{f}, \textit{x0}, \textit{ranges}={\tt None}, \textit{h}={\tt 0.5}, \textit{emax}={\tt 1e-08}, \textit{imax}={\tt 1000})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Initializes the optimizer.

To create an optimizer of this type, instantiate the class with the
parameters given below:
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxxx}

          \item[f]


A multivariable function to be optimized. The function should have
only one parameter, a multidimensional line-vector, and return the
function value, a scalar.
          \item[x0]


First estimate of the minimum. Estimates can be given in any format,
but internally they are converted to a one-dimension vector, where
each component corresponds to the estimate of that particular
variable. The vector is computed by flattening the array.
          \item[ranges]


A range of values might be passed to the algorithm, but it is not
necessary. If supplied, this parameter should be a list of ranges
for each variable of the objective function. It is specified as a
list of tuples of two values, \texttt{(x0, x1)}, where \texttt{x0} is the
start of the interval, and \texttt{x1} its end. Obviously, \texttt{x0} should
be smaller than \texttt{x1}. It can also be given as a list with a simple
tuple in the same format. In that case, the same range will be
applied for every variable in the optimization.
          \item[h]


The initial step of the search. Defaults to 0.5
          \item[emax]


Maximum allowed error. The algorithm stops as soon as the error is
below this level. The error is absolute.
          \item[imax]


Maximum number of iterations, the algorithm stops as soon this
number of iterations are executed, no matter what the error is at
the moment.
        \end{Ventry}

      \end{quote}

      Overrides: object.\_\_init\_\_

    \end{boxedminipage}

    \label{peach:optm:multivar:Direct:restart}
    \index{peach \textit{(package)}!peach.optm \textit{(package)}!peach.optm.multivar \textit{(module)}!peach.optm.multivar.Direct \textit{(class)}!peach.optm.multivar.Direct.restart \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{restart}(\textit{self}, \textit{x0}, \textit{h}={\tt 0.5})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Resets the optimizer, returning to its original state, and allowing to
use a new first estimate.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xx}

          \item[x0]


New estimate of the minimum. Estimates can be given in any format,
but internally they are converted to a one-dimension vector, where
each component corresponds to the estimate of that particular
variable. The vector is computed by flattening the array.
          \item[h]


The initial step of the search. Defaults to 0.5
        \end{Ventry}

      \end{quote}

    \end{boxedminipage}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{step}(\textit{self})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

One step of the search.

In this method, the result of the step is highly dependent of the steps
executed before, as the search step is updated at each call to this
method.
\setlength{\parskip}{1ex}
      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

This method returns a tuple \texttt{(x, e)}, where \texttt{x} is the updated
estimate of the minimum, and \texttt{e} is the estimated error.
      \end{quote}

      Overrides: peach.optm.base.Optimizer.step

    \end{boxedminipage}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_call\_\_}(\textit{self})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Transparently executes the search until the minimum is found. The stop
criteria are the maximum error or the maximum number of iterations,
whichever is reached first. Note that this is a \texttt{\_\_call\_\_} method, so
the object is called as a function. This method returns a tuple
\texttt{(x, e)}, with the best estimate of the minimum and the error.
\setlength{\parskip}{1ex}
      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

This method returns a tuple \texttt{(x, e)}, where \texttt{x} is the best
estimate of the minimum, and \texttt{e} is the estimated error.
      \end{quote}

      Overrides: peach.optm.base.Optimizer.\_\_call\_\_

    \end{boxedminipage}


\large{\textbf{\textit{Inherited from object}}}

\begin{quote}
\_\_delattr\_\_(), \_\_format\_\_(), \_\_getattribute\_\_(), \_\_hash\_\_(), \_\_new\_\_(), \_\_reduce\_\_(), \_\_reduce\_ex\_\_(), \_\_repr\_\_(), \_\_setattr\_\_(), \_\_sizeof\_\_(), \_\_str\_\_(), \_\_subclasshook\_\_()
\end{quote}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                              Properties                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Properties}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright x\- & &\\
\cline{1-2}
\multicolumn{2}{|l|}{\textit{Inherited from object}}\\
\multicolumn{2}{|p{\varwidth}|}{\raggedright \_\_class\_\_}\\
\cline{1-2}
\end{longtable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                          Instance Variables                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Instance Variables}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright r\-a\-n\-g\-e\-s\- & Holds the ranges for every variable. Although it is a writable
property, care should be taken in changing parameters before ending the
convergence.&\\
\cline{1-2}
\end{longtable}

    \index{peach \textit{(package)}!peach.optm \textit{(package)}!peach.optm.multivar \textit{(module)}!peach.optm.multivar.Direct \textit{(class)}|)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           Class Description                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \index{peach \textit{(package)}!peach.optm \textit{(package)}!peach.optm.multivar \textit{(module)}!peach.optm.multivar.Gradient \textit{(class)}|(}
\subsection{Class Gradient}

    \label{peach:optm:multivar:Gradient}
\begin{tabular}{cccccccc}
% Line for object, linespec=[False, False]
\multicolumn{2}{r}{\settowidth{\BCL}{object}\multirow{2}{\BCL}{object}}
&&
&&
  \\\cline{3-3}
  &&\multicolumn{1}{c|}{}
&&
&&
  \\
% Line for peach.optm.base.Optimizer, linespec=[False]
\multicolumn{4}{r}{\settowidth{\BCL}{peach.optm.base.Optimizer}\multirow{2}{\BCL}{peach.optm.base.Optimizer}}
&&
  \\\cline{5-5}
  &&&&\multicolumn{1}{c|}{}
&&
  \\
&&&&\multicolumn{2}{l}{\textbf{peach.optm.multivar.Gradient}}
\end{tabular}


Gradient search

This method uses the fact that the gradient of a function points to the
direction of largest increase in the function (in general called \emph{uphill}
direction). So, the contrary direction (\emph{downhill}) is used as search
direction.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                Methods                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Methods}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_init\_\_}(\textit{self}, \textit{f}, \textit{x0}, \textit{ranges}={\tt None}, \textit{df}={\tt None}, \textit{h}={\tt 0.1}, \textit{emax}={\tt 1e-05}, \textit{imax}={\tt 1000})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Initializes the optimizer.

To create an optimizer of this type, instantiate the class with the
parameters given below:
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxxx}

          \item[f]


A multivariable function to be optimized. The function should have
only one parameter, a multidimensional line-vector, and return the
function value, a scalar.
          \item[x0]


First estimate of the minimum. Estimates can be given in any format,
but internally they are converted to a one-dimension vector, where
each component corresponds to the estimate of that particular
variable. The vector is computed by flattening the array.
          \item[ranges]


A range of values might be passed to the algorithm, but it is not
necessary. If supplied, this parameter should be a list of ranges
for each variable of the objective function. It is specified as a
list of tuples of two values, \texttt{(x0, x1)}, where \texttt{x0} is the
start of the interval, and \texttt{x1} its end. Obviously, \texttt{x0} should
be smaller than \texttt{x1}. It can also be given as a list with a simple
tuple in the same format. In that case, the same range will be
applied for every variable in the optimization.
          \item[df]


A function to calculate the gradient vector of the cost function
\texttt{f}. Defaults to \texttt{None}, if no gradient is supplied, then it is
estimated from the cost function using Euler equations.
          \item[h]


Convergence step. This method does not takes into consideration the
possibility of varying the convergence step, to avoid Stiefel cages.
          \item[emax]


Maximum allowed error. The algorithm stops as soon as the error is
below this level. The error is absolute.
          \item[imax]


Maximum number of iterations, the algorithm stops as soon this
number of iterations are executed, no matter what the error is at
the moment.
        \end{Ventry}

      \end{quote}

      Overrides: object.\_\_init\_\_

    \end{boxedminipage}

    \label{peach:optm:multivar:Gradient:restart}
    \index{peach \textit{(package)}!peach.optm \textit{(package)}!peach.optm.multivar \textit{(module)}!peach.optm.multivar.Gradient \textit{(class)}!peach.optm.multivar.Gradient.restart \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{restart}(\textit{self}, \textit{x0}, \textit{h}={\tt None})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Resets the optimizer, returning to its original state, and allowing to
use a new first estimate.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xx}

          \item[x0]


New estimate of the minimum. Estimates can be given in any format,
but internally they are converted to a one-dimension vector, where
each component corresponds to the estimate of that particular
variable. The vector is computed by flattening the array.
          \item[h]


Convergence step. This method does not takes into consideration the
possibility of varying the convergence step, to avoid Stiefel cages.
        \end{Ventry}

      \end{quote}

    \end{boxedminipage}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{step}(\textit{self})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

One step of the search.

In this method, the result of the step is dependent only of the given
estimated, so it can be used for different kind of investigations on the
same cost function.
\setlength{\parskip}{1ex}
      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

This method returns a tuple \texttt{(x, e)}, where \texttt{x} is the updated
estimate of the minimum, and \texttt{e} is the estimated error.
      \end{quote}

      Overrides: peach.optm.base.Optimizer.step

    \end{boxedminipage}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_call\_\_}(\textit{self})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Transparently executes the search until the minimum is found. The stop
criteria are the maximum error or the maximum number of iterations,
whichever is reached first. Note that this is a \texttt{\_\_call\_\_} method, so
the object is called as a function. This method returns a tuple
\texttt{(x, e)}, with the best estimate of the minimum and the error.
\setlength{\parskip}{1ex}
      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

This method returns a tuple \texttt{(x, e)}, where \texttt{x} is the best
estimate of the minimum, and \texttt{e} is the estimated error.
      \end{quote}

      Overrides: peach.optm.base.Optimizer.\_\_call\_\_

    \end{boxedminipage}


\large{\textbf{\textit{Inherited from object}}}

\begin{quote}
\_\_delattr\_\_(), \_\_format\_\_(), \_\_getattribute\_\_(), \_\_hash\_\_(), \_\_new\_\_(), \_\_reduce\_\_(), \_\_reduce\_ex\_\_(), \_\_repr\_\_(), \_\_setattr\_\_(), \_\_sizeof\_\_(), \_\_str\_\_(), \_\_subclasshook\_\_()
\end{quote}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                              Properties                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Properties}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright x\- & &\\
\cline{1-2}
\multicolumn{2}{|l|}{\textit{Inherited from object}}\\
\multicolumn{2}{|p{\varwidth}|}{\raggedright \_\_class\_\_}\\
\cline{1-2}
\end{longtable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                          Instance Variables                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Instance Variables}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright r\-a\-n\-g\-e\-s\- & Holds the ranges for every variable. Although it is a writable
property, care should be taken in changing parameters before ending the
convergence.&\\
\cline{1-2}
\end{longtable}

    \index{peach \textit{(package)}!peach.optm \textit{(package)}!peach.optm.multivar \textit{(module)}!peach.optm.multivar.Gradient \textit{(class)}|)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           Class Description                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \index{peach \textit{(package)}!peach.optm \textit{(package)}!peach.optm.multivar \textit{(module)}!peach.optm.multivar.MomentumGradient \textit{(class)}|(}
\subsection{Class MomentumGradient}

    \label{peach:optm:multivar:MomentumGradient}
\begin{tabular}{cccccccc}
% Line for object, linespec=[False, False]
\multicolumn{2}{r}{\settowidth{\BCL}{object}\multirow{2}{\BCL}{object}}
&&
&&
  \\\cline{3-3}
  &&\multicolumn{1}{c|}{}
&&
&&
  \\
% Line for peach.optm.base.Optimizer, linespec=[False]
\multicolumn{4}{r}{\settowidth{\BCL}{peach.optm.base.Optimizer}\multirow{2}{\BCL}{peach.optm.base.Optimizer}}
&&
  \\\cline{5-5}
  &&&&\multicolumn{1}{c|}{}
&&
  \\
&&&&\multicolumn{2}{l}{\textbf{peach.optm.multivar.MomentumGradient}}
\end{tabular}


Gradient search with momentum

This method uses the fact that the gradient of a function points to the
direction of largest increase in the function (in general called \emph{uphill}
direction). So, the contrary direction (\emph{downhill}) is used as search
direction. A momentum term is added to avoid local minima.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                Methods                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Methods}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_init\_\_}(\textit{self}, \textit{f}, \textit{x0}, \textit{ranges}={\tt None}, \textit{df}={\tt None}, \textit{h}={\tt 0.1}, \textit{a}={\tt 0.1}, \textit{emax}={\tt 1e-05}, \textit{imax}={\tt 1000})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Initializes the optimizer.

To create an optimizer of this type, instantiate the class with the
parameters given below:
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxxx}

          \item[f]


A multivariable function to be optimized. The function should have
only one parameter, a multidimensional line-vector, and return the
function value, a scalar.
          \item[x0]


First estimate of the minimum. Estimates can be given in any format,
but internally they are converted to a one-dimension vector, where
each component corresponds to the estimate of that particular
variable. The vector is computed by flattening the array.
          \item[ranges]


A range of values might be passed to the algorithm, but it is not
necessary. If supplied, this parameter should be a list of ranges
for each variable of the objective function. It is specified as a
list of tuples of two values, \texttt{(x0, x1)}, where \texttt{x0} is the
start of the interval, and \texttt{x1} its end. Obviously, \texttt{x0} should
be smaller than \texttt{x1}. It can also be given as a list with a simple
tuple in the same format. In that case, the same range will be
applied for every variable in the optimization.
          \item[df]


A function to calculate the gradient vector of the cost function
\texttt{f}. Defaults to \texttt{None}, if no gradient is supplied, then it is
estimated from the cost function using Euler equations.
          \item[h]


Convergence step. This method does not takes into consideration the
possibility of varying the convergence step, to avoid Stiefel cages.
Defaults to 0.1.
          \item[a]


Momentum term. This term is a measure of the memory of the optmizer.
The bigger it is, the more the past values influence in the outcome
of the optimization. Defaults to 0.1
          \item[emax]


Maximum allowed error. The algorithm stops as soon as the error is
below this level. The error is absolute.
          \item[imax]


Maximum number of iterations, the algorithm stops as soon this
number of iterations are executed, no matter what the error is at
the moment.
        \end{Ventry}

      \end{quote}

      Overrides: object.\_\_init\_\_

    \end{boxedminipage}

    \label{peach:optm:multivar:MomentumGradient:restart}
    \index{peach \textit{(package)}!peach.optm \textit{(package)}!peach.optm.multivar \textit{(module)}!peach.optm.multivar.MomentumGradient \textit{(class)}!peach.optm.multivar.MomentumGradient.restart \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{restart}(\textit{self}, \textit{x0}, \textit{h}={\tt None}, \textit{a}={\tt None})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Resets the optimizer, returning to its original state, and allowing to
use a new first estimate.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xx}

          \item[x0]


New estimate of the minimum. Estimates can be given in any format,
but internally they are converted to a one-dimension vector, where
each component corresponds to the estimate of that particular
variable. The vector is computed by flattening the array.
          \item[h]


Convergence step. This method does not takes into consideration the
possibility of varying the convergence step, to avoid Stiefel cages.
If not given in this method, the old value is used.
          \item[a]


Momentum term. This term is a measure of the memory of the optmizer.
The bigger it is, the more the past values influence in the outcome
of the optimization. If not given in this method, the old value is
used.
        \end{Ventry}

      \end{quote}

    \end{boxedminipage}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{step}(\textit{self})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

One step of the search.

In this method, the result of the step is dependent only of the given
estimated, so it can be used for different kind of investigations on the
same cost function.
\setlength{\parskip}{1ex}
      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

This method returns a tuple \texttt{(x, e)}, where \texttt{x} is the updated
estimate of the minimum, and \texttt{e} is the estimated error.
      \end{quote}

      Overrides: peach.optm.base.Optimizer.step

    \end{boxedminipage}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_call\_\_}(\textit{self})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Transparently executes the search until the minimum is found. The stop
criteria are the maximum error or the maximum number of iterations,
whichever is reached first. Note that this is a \texttt{\_\_call\_\_} method, so
the object is called as a function. This method returns a tuple
\texttt{(x, e)}, with the best estimate of the minimum and the error.
\setlength{\parskip}{1ex}
      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

This method returns a tuple \texttt{(x, e)}, where \texttt{x} is the best
estimate of the minimum, and \texttt{e} is the estimated error.
      \end{quote}

      Overrides: peach.optm.base.Optimizer.\_\_call\_\_

    \end{boxedminipage}


\large{\textbf{\textit{Inherited from object}}}

\begin{quote}
\_\_delattr\_\_(), \_\_format\_\_(), \_\_getattribute\_\_(), \_\_hash\_\_(), \_\_new\_\_(), \_\_reduce\_\_(), \_\_reduce\_ex\_\_(), \_\_repr\_\_(), \_\_setattr\_\_(), \_\_sizeof\_\_(), \_\_str\_\_(), \_\_subclasshook\_\_()
\end{quote}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                              Properties                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Properties}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright x\- & &\\
\cline{1-2}
\multicolumn{2}{|l|}{\textit{Inherited from object}}\\
\multicolumn{2}{|p{\varwidth}|}{\raggedright \_\_class\_\_}\\
\cline{1-2}
\end{longtable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                          Instance Variables                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Instance Variables}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright r\-a\-n\-g\-e\-s\- & Holds the ranges for every variable. Although it is a writable
property, care should be taken in changing parameters before ending the
convergence.&\\
\cline{1-2}
\end{longtable}

    \index{peach \textit{(package)}!peach.optm \textit{(package)}!peach.optm.multivar \textit{(module)}!peach.optm.multivar.MomentumGradient \textit{(class)}|)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           Class Description                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \index{peach \textit{(package)}!peach.optm \textit{(package)}!peach.optm.multivar \textit{(module)}!peach.optm.multivar.Newton \textit{(class)}|(}
\subsection{Class Newton}

    \label{peach:optm:multivar:Newton}
\begin{tabular}{cccccccc}
% Line for object, linespec=[False, False]
\multicolumn{2}{r}{\settowidth{\BCL}{object}\multirow{2}{\BCL}{object}}
&&
&&
  \\\cline{3-3}
  &&\multicolumn{1}{c|}{}
&&
&&
  \\
% Line for peach.optm.base.Optimizer, linespec=[False]
\multicolumn{4}{r}{\settowidth{\BCL}{peach.optm.base.Optimizer}\multirow{2}{\BCL}{peach.optm.base.Optimizer}}
&&
  \\\cline{5-5}
  &&&&\multicolumn{1}{c|}{}
&&
  \\
&&&&\multicolumn{2}{l}{\textbf{peach.optm.multivar.Newton}}
\end{tabular}


Newton search

This is a very effective method to find minimum points in functions. In a
very basic fashion, this method corresponds to using Newton root finding
method on f'(x). Converges \emph{very} fast if the cost function is quadratic
of similar to it.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                Methods                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Methods}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_init\_\_}(\textit{self}, \textit{f}, \textit{x0}, \textit{ranges}={\tt None}, \textit{df}={\tt None}, \textit{hf}={\tt None}, \textit{h}={\tt 0.1}, \textit{emax}={\tt 1e-05}, \textit{imax}={\tt 1000})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Initializes the optimizer.

To create an optimizer of this type, instantiate the class with the
parameters given below:
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxxx}

          \item[f]


A multivariable function to be optimized. The function should have
only one parameter, a multidimensional line-vector, and return the
function value, a scalar.
          \item[x0]


First estimate of the minimum. Estimates can be given in any format,
but internally they are converted to a one-dimension vector, where
each component corresponds to the estimate of that particular
variable. The vector is computed by flattening the array.
          \item[ranges]


A range of values might be passed to the algorithm, but it is not
necessary. If supplied, this parameter should be a list of ranges
for each variable of the objective function. It is specified as a
list of tuples of two values, \texttt{(x0, x1)}, where \texttt{x0} is the
start of the interval, and \texttt{x1} its end. Obviously, \texttt{x0} should
be smaller than \texttt{x1}. It can also be given as a list with a simple
tuple in the same format. In that case, the same range will be
applied for every variable in the optimization.
          \item[df]


A function to calculate the gradient vector of the cost function
\texttt{f}. Defaults to \texttt{None}, if no gradient is supplied, then it is
estimated from the cost function using Euler equations.
          \item[hf]


A function to calculate the hessian matrix of the cost function
\texttt{f}. Defaults to \texttt{None}, if no hessian is supplied, then it is
estimated from the cost function using Euler equations.
          \item[h]


Convergence step. This method does not takes into consideration the
possibility of varying the convergence step, to avoid Stiefel cages.
          \item[emax]


Maximum allowed error. The algorithm stops as soon as the error is
below this level. The error is absolute.
          \item[imax]


Maximum number of iterations, the algorithm stops as soon this
number of iterations are executed, no matter what the error is at
the moment.
        \end{Ventry}

      \end{quote}

      Overrides: object.\_\_init\_\_

    \end{boxedminipage}

    \label{peach:optm:multivar:Newton:restart}
    \index{peach \textit{(package)}!peach.optm \textit{(package)}!peach.optm.multivar \textit{(module)}!peach.optm.multivar.Newton \textit{(class)}!peach.optm.multivar.Newton.restart \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{restart}(\textit{self}, \textit{x0}, \textit{h}={\tt None})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Resets the optimizer, returning to its original state, and allowing to
use a new first estimate.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xx}

          \item[x0]


New estimate of the minimum. Estimates can be given in any format,
but internally they are converted to a one-dimension vector, where
each component corresponds to the estimate of that particular
variable. The vector is computed by flattening the array.
          \item[h]


Convergence step. This method does not takes into consideration the
possibility of varying the convergence step, to avoid Stiefel cages.
        \end{Ventry}

      \end{quote}

    \end{boxedminipage}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{step}(\textit{self})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

One step of the search.

In this method, the result of the step is dependent only of the given
estimated, so it can be used for different kind of investigations on the
same cost function.
\setlength{\parskip}{1ex}
      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

This method returns a tuple \texttt{(x, e)}, where \texttt{x} is the updated
estimate of the minimum, and \texttt{e} is the estimated error.
      \end{quote}

      Overrides: peach.optm.base.Optimizer.step

    \end{boxedminipage}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_call\_\_}(\textit{self})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Transparently executes the search until the minimum is found. The stop
criteria are the maximum error or the maximum number of iterations,
whichever is reached first. Note that this is a \texttt{\_\_call\_\_} method, so
the object is called as a function. This method returns a tuple
\texttt{(x, e)}, with the best estimate of the minimum and the error.
\setlength{\parskip}{1ex}
      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

This method returns a tuple \texttt{(x, e)}, where \texttt{x} is the best
estimate of the minimum, and \texttt{e} is the estimated error.
      \end{quote}

      Overrides: peach.optm.base.Optimizer.\_\_call\_\_

    \end{boxedminipage}


\large{\textbf{\textit{Inherited from object}}}

\begin{quote}
\_\_delattr\_\_(), \_\_format\_\_(), \_\_getattribute\_\_(), \_\_hash\_\_(), \_\_new\_\_(), \_\_reduce\_\_(), \_\_reduce\_ex\_\_(), \_\_repr\_\_(), \_\_setattr\_\_(), \_\_sizeof\_\_(), \_\_str\_\_(), \_\_subclasshook\_\_()
\end{quote}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                              Properties                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Properties}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright x\- & &\\
\cline{1-2}
\multicolumn{2}{|l|}{\textit{Inherited from object}}\\
\multicolumn{2}{|p{\varwidth}|}{\raggedright \_\_class\_\_}\\
\cline{1-2}
\end{longtable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                          Instance Variables                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Instance Variables}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright r\-a\-n\-g\-e\-s\- & Holds the ranges for every variable. Although it is a writable
property, care should be taken in changing parameters before ending the
convergence.&\\
\cline{1-2}
\end{longtable}

    \index{peach \textit{(package)}!peach.optm \textit{(package)}!peach.optm.multivar \textit{(module)}!peach.optm.multivar.Newton \textit{(class)}|)}
    \index{peach \textit{(package)}!peach.optm \textit{(package)}!peach.optm.multivar \textit{(module)}|)}
