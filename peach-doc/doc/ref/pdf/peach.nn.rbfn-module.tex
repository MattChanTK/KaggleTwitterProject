%
% API Documentation for Peach - Computational Intelligence for Python
% Module peach.nn.rbfn
%
% Generated by epydoc 3.0.1
% [Sun Jul 31 17:00:41 2011]
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                          Module Description                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.rbfn \textit{(module)}|(}
\section{Module peach.nn.rbfn}

    \label{peach:nn:rbfn}

Radial Basis Function Networks

This sub-package implements the basic behaviour of radial basis function
networks. This is a two-layer neural network that works as a universal function
approximator. The activation functions of the first layer are radial basis
functions (RBFs), that are symmetric around the origin, that is, the value of
this kind of function depends only on the distance of the evaluated point to the
origin. The second layer has only one neuron with linear activation, that is, it
only combines the inputs of the first layer.

The training of this kind of network, while it can be done using a traditional
optimization technique such as gradient descent, is usually made in two steps.
In the first step, the position of the centers and the width of the RBFs are
computed. In the second step, the weights of the second layer are adapted. In
this module, the RBFN architecture is implemented, allowing training of the
second layer. Centers must be supplied, but they can be easily found from the
training set using algorithms such as K-Means (the one traditionally used),
SOMs or Fuzzy C-Means.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               Functions                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Functions}

    \label{peach:nn:rbfn:randn}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.rbfn \textit{(module)}!peach.nn.rbfn.randn \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{randn}(\textit{d0}, \textit{d1}, \textit{dn}, \textit{...})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Returns zero-mean, unit-variance Gaussian random numbers in an
array of shape (d0, d1, ..., dn).
%
\begin{description}
\item[{Note:  This is a convenience function. If you want an}] \leavevmode 
interface that takes a tuple as the first argument
use numpy.random.standard\_normal(shape\_tuple).

\end{description}
\setlength{\parskip}{1ex}
    \end{boxedminipage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               Variables                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Variables}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright \_\-\_\-d\-o\-c\-\_\-\_\- & \raggedright \textbf{Value:} 
{\tt \texttt{...}}&\\
\cline{1-2}
\raggedright \_\-\_\-p\-a\-c\-k\-a\-g\-e\-\_\-\_\- & \raggedright \textbf{Value:} 
{\tt \texttt{'}\texttt{peach.nn}\texttt{'}}&\\
\cline{1-2}
\raggedright a\-b\-s\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'absolute'{\textgreater}}&\\
\cline{1-2}
\raggedright a\-r\-c\-t\-a\-n\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'arctan'{\textgreater}}&\\
\cline{1-2}
\raggedright c\-o\-s\-h\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'cosh'{\textgreater}}&\\
\cline{1-2}
\raggedright e\-x\-p\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'exp'{\textgreater}}&\\
\cline{1-2}
\raggedright p\-i\- & \raggedright \textbf{Value:} 
{\tt 3.14159265359}&\\
\cline{1-2}
\raggedright s\-i\-g\-n\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'sign'{\textgreater}}&\\
\cline{1-2}
\raggedright s\-q\-r\-t\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'sqrt'{\textgreater}}&\\
\cline{1-2}
\raggedright t\-a\-n\-h\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'tanh'{\textgreater}}&\\
\cline{1-2}
\end{longtable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           Class Description                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.rbfn \textit{(module)}!peach.nn.rbfn.RBFN \textit{(class)}|(}
\subsection{Class RBFN}

    \label{peach:nn:rbfn:RBFN}
\begin{tabular}{cccccc}
% Line for object, linespec=[False]
\multicolumn{2}{r}{\settowidth{\BCL}{object}\multirow{2}{\BCL}{object}}
&&
  \\\cline{3-3}
  &&\multicolumn{1}{c|}{}
&&
  \\
&&\multicolumn{2}{l}{\textbf{peach.nn.rbfn.RBFN}}
\end{tabular}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                Methods                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Methods}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_init\_\_}(\textit{self}, \textit{c}, \textit{phi}={\tt {\textless}class 'peach.nn.af.Gaussian'{\textgreater}}, \textit{phi2}={\tt {\textless}class 'peach.nn.af.Linear'{\textgreater}})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Initializes the radial basis function network.

A radial basis function is implemented as two layers of neurons, the
first one with the RBFs, the second one a linear combinator.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxx}

          \item[c]


Two-dimensional array containing the centers of the radial basis
functions, where each line is a vector with the components of the
center. Thus, the number of lines in this array is the number of
centers of the network.
          \item[phi]


The radial basis function to be used in the first layer. Defaults to
the gaussian.
          \item[phi2]


The activation function of the second layer. If the network is being
used to approximate functions, this should be Linear. Since this is
the most commom situation, it is the default value. In occasions,
this can be made (say) a sigmoid, for pattern recognition.
        \end{Ventry}

      \end{quote}

      Overrides: object.\_\_init\_\_

    \end{boxedminipage}

    \label{peach:nn:rbfn:RBFN:__call__}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.rbfn \textit{(module)}!peach.nn.rbfn.RBFN \textit{(class)}!peach.nn.rbfn.RBFN.\_\_call\_\_ \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_call\_\_}(\textit{self}, \textit{x})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Feeds the network and return the result.

The \texttt{\_\_call\_\_} interface should be called if the answer of the neuron
network to a given input vector \texttt{x} is desired. \emph{This method has
collateral effects}, so beware. After the calling of this method, the
\texttt{y} property is set with the activation potential and the answer of
the neurons, respectivelly.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


The input vector to the network.
        \end{Ventry}

      \end{quote}

      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

The vector containing the answer of every neuron in the last layer, in
the respective order.
      \end{quote}

    \end{boxedminipage}

    \label{peach:nn:rbfn:RBFN:learn}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.rbfn \textit{(module)}!peach.nn.rbfn.RBFN \textit{(class)}!peach.nn.rbfn.RBFN.learn \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{learn}(\textit{self}, \textit{x}, \textit{d})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Applies one example of the training set to the network.

Using this method, one iteration of the learning procedure is executed
for the second layer of the network. This method presents one example
(not necessarilly from a training set) and applies the learning rule
over the layer. The learning rule is defined in the initialization of
the network, and some are implemented on the \texttt{lrules} method. New
methods can be created, consult the \texttt{lrules} documentation but, for
the second layer of a \texttt{RBFN'{}' instance, only `{}`FFLearning} learning is
allowed.

Also, notice that \emph{this method only applies the learning method!} The
network should be fed with the same input vector before trying to learn
anything first. Consult the \texttt{feed} and \texttt{train} methods below for
more ways to train a network.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


Input vector of the example. It should be a column vector of the
correct dimension, that is, the number of input neurons.
          \item[d]


The desired answer of the network for this particular input vector.
Notice that the desired answer should have the same dimension of the
last layer of the network. This means that a desired answer should
be given for every output of the network.
        \end{Ventry}

      \end{quote}

      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

The error obtained by the network.
      \end{quote}

    \end{boxedminipage}

    \label{peach:nn:rbfn:RBFN:feed}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.rbfn \textit{(module)}!peach.nn.rbfn.RBFN \textit{(class)}!peach.nn.rbfn.RBFN.feed \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{feed}(\textit{self}, \textit{x}, \textit{d})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Feed the network and applies one example of the training set to the
network. This adapts only the synaptic weights in the second layer of
the RBFN.

Using this method, one iteration of the learning procedure is made with
the neurons of this network. This method presents one example (not
necessarilly from a training set) and applies the learning rule over the
network. The learning rule is defined in the initialization of the
network, and some are implemented on the \texttt{lrules} method. New methods
can be created, consult the \texttt{lrules} documentation but, for the second
layer of a \texttt{RBFN}, only \texttt{FFLearning} learning is allowed.

Also, notice that \emph{this method feeds the network} before applying the
learning rule. Feeding the network has collateral effects, and some
properties change when this happens. Namely, the \texttt{y} property is set.
Please consult the \texttt{\_\_call\_\_} interface.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


Input vector of the example. It should be a column vector of the
correct dimension, that is, the number of input neurons.
          \item[d]


The desired answer of the network for this particular input vector.
Notice that the desired answer should have the same dimension of the
last layer of the network. This means that a desired answer should
be given for every output of the network.
        \end{Ventry}

      \end{quote}

      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

The error obtained by the network.
      \end{quote}

    \end{boxedminipage}

    \label{peach:nn:rbfn:RBFN:train}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.rbfn \textit{(module)}!peach.nn.rbfn.RBFN \textit{(class)}!peach.nn.rbfn.RBFN.train \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{train}(\textit{self}, \textit{train\_set}, \textit{imax}={\tt 2000}, \textit{emax}={\tt 1e-05}, \textit{randomize}={\tt False})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Presents a training set to the network.

This method automatizes the training of the network. Given a training
set, the examples are shown to the network (possibly in a randomized
way). A maximum number of iterations or a maximum admitted error should
be given as a stop condition.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxxxxxx}

          \item[train\_set]


The training set is a list of examples. It can have any size and can
contain repeated examples. In fact, the definition of the training
set is open. Each element of the training set, however, should be a
two-tuple \texttt{(x, d)}, where \texttt{x} is the input vector, and \texttt{d} is
the desired response of the network for this particular input. See
the \texttt{learn} and \texttt{feed} for more information.
          \item[imax]


The maximum number of iterations. Examples from the training set
will be presented to the network while this limit is not reached.
Defaults to 2000.
          \item[emax]


The maximum admitted error. Examples from the training set will be
presented to the network until the error obtained is lower than this
limit. Defaults to 1e-5.
          \item[randomize]


If this is \texttt{True}, then the examples are shown in a randomized
order. If \texttt{False}, then the examples are shown in the same order
that they appear in the \texttt{train\_set} list. Defaults to \texttt{False}.
        \end{Ventry}

      \end{quote}

    \end{boxedminipage}


\large{\textbf{\textit{Inherited from object}}}

\begin{quote}
\_\_delattr\_\_(), \_\_format\_\_(), \_\_getattribute\_\_(), \_\_hash\_\_(), \_\_new\_\_(), \_\_reduce\_\_(), \_\_reduce\_ex\_\_(), \_\_repr\_\_(), \_\_setattr\_\_(), \_\_sizeof\_\_(), \_\_str\_\_(), \_\_subclasshook\_\_()
\end{quote}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                              Properties                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Properties}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright w\-i\-d\-t\-h\- & &\\
\cline{1-2}
\raggedright w\-e\-i\-g\-h\-t\-s\- & &\\
\cline{1-2}
\raggedright y\- & &\\
\cline{1-2}
\raggedright p\-h\-i\- & &\\
\cline{1-2}
\raggedright p\-h\-i\-2\- & &\\
\cline{1-2}
\multicolumn{2}{|l|}{\textit{Inherited from object}}\\
\multicolumn{2}{|p{\varwidth}|}{\raggedright \_\_class\_\_}\\
\cline{1-2}
\end{longtable}

    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.rbfn \textit{(module)}!peach.nn.rbfn.RBFN \textit{(class)}|)}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.rbfn \textit{(module)}|)}
