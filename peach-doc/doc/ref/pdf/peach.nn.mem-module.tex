%
% API Documentation for Peach - Computational Intelligence for Python
% Module peach.nn.mem
%
% Generated by epydoc 3.0.1
% [Sun Jul 31 17:00:41 2011]
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                          Module Description                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.mem \textit{(module)}|(}
\section{Module peach.nn.mem}

    \label{peach:nn:mem}

Associative memories and Hopfield network model.

This sub-package implements associative memories. In associative memories,
information is recovered by supplying not an exact index (such as in their
usual counterparts), but supplying an index simmilar enough that the information
can be deduced from what is stored in its synaptic weights. There are a number
of different memories of this kind.

The most common type is the Hopfield network. A Hopfield network is a recurrent
self-associative memory. Although there are real-valued versions of the network,
the binary type is more common. In it, patterns are recovered from an initial
estimate through an iterative process.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               Functions                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Functions}

    \label{peach:nn:rbfn:randn}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.rbfn \textit{(module)}!peach.nn.rbfn.randn \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{randn}(\textit{d0}, \textit{d1}, \textit{dn}, \textit{...})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Returns zero-mean, unit-variance Gaussian random numbers in an
array of shape (d0, d1, ..., dn).
%
\begin{description}
\item[{Note:  This is a convenience function. If you want an}] \leavevmode 
interface that takes a tuple as the first argument
use numpy.random.standard\_normal(shape\_tuple).

\end{description}
\setlength{\parskip}{1ex}
    \end{boxedminipage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               Variables                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Variables}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright \_\-\_\-d\-o\-c\-\_\-\_\- & \raggedright \textbf{Value:} 
{\tt \texttt{...}}&\\
\cline{1-2}
\raggedright \_\-\_\-p\-a\-c\-k\-a\-g\-e\-\_\-\_\- & \raggedright \textbf{Value:} 
{\tt \texttt{'}\texttt{peach.nn}\texttt{'}}&\\
\cline{1-2}
\raggedright a\-r\-c\-t\-a\-n\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'arctan'{\textgreater}}&\\
\cline{1-2}
\raggedright c\-o\-s\-h\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'cosh'{\textgreater}}&\\
\cline{1-2}
\raggedright e\-x\-p\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'exp'{\textgreater}}&\\
\cline{1-2}
\raggedright p\-i\- & \raggedright \textbf{Value:} 
{\tt 3.14159265359}&\\
\cline{1-2}
\raggedright s\-i\-g\-n\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'sign'{\textgreater}}&\\
\cline{1-2}
\raggedright t\-a\-n\-h\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'tanh'{\textgreater}}&\\
\cline{1-2}
\end{longtable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           Class Description                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.mem \textit{(module)}!peach.nn.mem.Hopfield \textit{(class)}|(}
\subsection{Class Hopfield}

    \label{peach:nn:mem:Hopfield}
\begin{tabular}{cccccccc}
% Line for object, linespec=[False, False]
\multicolumn{2}{r}{\settowidth{\BCL}{object}\multirow{2}{\BCL}{object}}
&&
&&
  \\\cline{3-3}
  &&\multicolumn{1}{c|}{}
&&
&&
  \\
% Line for peach.nn.base.Layer, linespec=[False]
\multicolumn{4}{r}{\settowidth{\BCL}{peach.nn.base.Layer}\multirow{2}{\BCL}{peach.nn.base.Layer}}
&&
  \\\cline{5-5}
  &&&&\multicolumn{1}{c|}{}
&&
  \\
&&&&\multicolumn{2}{l}{\textbf{peach.nn.mem.Hopfield}}
\end{tabular}


Hopfield neural network model

A Hopfield network is a recurrent network of one layer of neurons. There
output of every neuron is conected to the inputs of every other neuron, but
not to itself. This kind of network is autoassociative, or content-based
memory. That means that, given a noisy version of a pattern stored in it,
the network is capable of, through an iterative algorithm, recover the
original pattern, removing the noise. There is a limit in the quantity of
patterns that can be stored without causing error, and if a pattern is
stored, its negated form is also stored.

This is the binary form of the Hopfield network, which is the most common
form. It implements a \texttt{Layer} of neurons, without bias, and with the
Signum as the activation function.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                Methods                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Methods}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_init\_\_}(\textit{self}, \textit{size}, \textit{phi}={\tt {\textless}class 'peach.nn.af.Signum'{\textgreater}})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Initializes the Hopfield network.

The Hopfield network is implemented as a layer of neurons.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxx}

          \item[size]


The number of neurons in the network. In a Hopfield network, the
number of neurons is also the number of inputs in each neuron, and
the dimensionality of the patterns to be stored and recovered.
          \item[phi]


The activation function. Traditionally, the Hopfield network uses
the signum function as activation. This is the default value.
        \end{Ventry}

      \end{quote}

      Overrides: object.\_\_init\_\_

    \end{boxedminipage}

    \label{peach:nn:mem:Hopfield:learn}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.mem \textit{(module)}!peach.nn.mem.Hopfield \textit{(class)}!peach.nn.mem.Hopfield.learn \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{learn}(\textit{self}, \textit{x})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Applies one example of the training set to the network.

Training a Hopfield network is not exactly an iterative procedure. The
network usually stores a small number of patterns, and the learning
procedure consists only in computing the synaptic weight matrix, which
can be done in very few steps (in fact, just the number of patterns).
This method is here for consistency with the rest of the library, but
it works, anyway.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


The pattern to be stored. It must be a vector with the same size as
the network, or else an exception will be raised. The pattern can be
of any dimensionality, but it will internally be converted to a
column vector.
        \end{Ventry}

      \end{quote}

    \end{boxedminipage}

    \label{peach:nn:mem:Hopfield:train}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.mem \textit{(module)}!peach.nn.mem.Hopfield \textit{(class)}!peach.nn.mem.Hopfield.train \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{train}(\textit{self}, \textit{train\_set})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Presents a training set to the network

This method stores all the patterns of the training set in the weight
matrix. It calls the \texttt{learn} method for every pattern in the set.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxxxxxx}

          \item[train\_set]


A list containing all the patterns to be stored in the network. Each
pattern is a vector of any dimensions, which are converted
internally to a column vector.
        \end{Ventry}

      \end{quote}

    \end{boxedminipage}

    \label{peach:nn:mem:Hopfield:step}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.mem \textit{(module)}!peach.nn.mem.Hopfield \textit{(class)}!peach.nn.mem.Hopfield.step \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{step}(\textit{self}, \textit{x})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Performs a step in the recovering procedure

The algorithm for recovering the patterns stored in a Hopfield network
is an iterative algorithm which goes from a starting test pattern (a
stored pattern with noise) and recovers the noiseless version -{}- if
possible. This method takes the test pattern and performs one step of
the convergence
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


The noisy pattern.
        \end{Ventry}

      \end{quote}

      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

The result of one step of the convergence. This might be the same as
the input pattern, or the pattern with one component inverted.
      \end{quote}

    \end{boxedminipage}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_call\_\_}(\textit{self}, \textit{x}, \textit{imax}={\tt 2000}, \textit{eqmax}={\tt 100})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Recovers a stored pattern

The \texttt{\_\_call\_\_} interface should be called if a memory needs to be
recovered from the network. Given a noisy pattern \texttt{x}, the algorithm
will be executed until convergence or a maximum number of iterations
occur. This method repeatedly calls the \texttt{step} method until a stop
condition is reached. The stop condition is the maximum number of
iterations, or a number of iterations where no changes are found in the
retrieved pattern.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxx}

          \item[x]


The noisy pattern vector presented to the network.
          \item[imax]


The maximum number of iterations the algorithm is to be repeated.
When this number of iterations is reached, the algorithm will stop,
whether the pattern was found or not. Defaults to 2000.
          \item[eqmax]


The maximum number of iterations the algorithm will be repeated if
no changes occur in the retrieval of the pattern. At each iteration
of the algorithm, a component might change. It is considered that,
if a number of iterations are performed and no changes are found in
the pattern, then the algorithm converged, and it stops. Defaults to
100.
        \end{Ventry}

      \end{quote}

      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

The vector containing the recovered pattern from the stored memories.
      \end{quote}

      Overrides: peach.nn.base.Layer.\_\_call\_\_

    \end{boxedminipage}


\large{\textbf{\textit{Inherited from peach.nn.base.Layer\textit{(Section \ref{peach:nn:base:Layer})}}}}

\begin{quote}
\_\_getitem\_\_(), \_\_setitem\_\_()
\end{quote}

\large{\textbf{\textit{Inherited from object}}}

\begin{quote}
\_\_delattr\_\_(), \_\_format\_\_(), \_\_getattribute\_\_(), \_\_hash\_\_(), \_\_new\_\_(), \_\_reduce\_\_(), \_\_reduce\_ex\_\_(), \_\_repr\_\_(), \_\_setattr\_\_(), \_\_sizeof\_\_(), \_\_str\_\_(), \_\_subclasshook\_\_()
\end{quote}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                              Properties                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Properties}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright i\-n\-p\-u\-t\-s\- & &\\
\cline{1-2}
\raggedright w\-e\-i\-g\-h\-t\-s\- & &\\
\cline{1-2}
\multicolumn{2}{|l|}{\textit{Inherited from peach.nn.base.Layer \textit{(Section \ref{peach:nn:base:Layer})}}}\\
\multicolumn{2}{|p{\varwidth}|}{\raggedright bias, phi, shape, size, v, y}\\
\cline{1-2}
\multicolumn{2}{|l|}{\textit{Inherited from object}}\\
\multicolumn{2}{|p{\varwidth}|}{\raggedright \_\_class\_\_}\\
\cline{1-2}
\end{longtable}

    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.mem \textit{(module)}!peach.nn.mem.Hopfield \textit{(class)}|)}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.mem \textit{(module)}|)}
